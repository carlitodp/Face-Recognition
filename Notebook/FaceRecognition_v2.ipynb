{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3180e69-fa09-49dc-b051-8bdd8d3dc6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./utility_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e565c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(devices = [\"/gpu:0\", \"/gpu:1\", \"/gpu:2\", \"/gpu:3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763cf8b9-91fd-4b6d-bc71-37ea2cda53df",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3878334",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (96,96,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5afdba-ed82-4c7d-86a9-d227a9daac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = \"../Dataset/clean/CelebrityFaces/\"\n",
    "valid_dataset_path = \"../Dataset/clean/FootballFaces/\"\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "trainLoader = Loader(path=train_dataset_path,\n",
    "                     image_shape=image_shape,\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     max_pos_pairs=3000,\n",
    "                     apply_augmentation=True,\n",
    "                     preprocess_pipeline = None,\n",
    "                     normalize = True)\n",
    "\n",
    "training_dataset = trainLoader.dataset\n",
    "\n",
    "validLoader = Loader(path=valid_dataset_path,\n",
    "                     image_shape=image_shape,\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     max_pos_pairs=500,\n",
    "                     preprocess_pipeline = None,\n",
    "                     normalize=True)\n",
    "\n",
    "validation_dataset = validLoader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe974f10-6e8b-4571-b3e8-c10a593bc2e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainLoader.visualize(value_range=(0,1),\n",
    "                      color_mode_switch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74f8ba5-6dee-46bc-a06b-f610019aba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dataset, class_names = validLoader.create_embedding_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffe6777-2a70-4f28-b4d6-814cf74e6f3e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf493d6-f74e-4455-828b-c60f9c20832d",
   "metadata": {},
   "source": [
    "## Simple Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373076aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f\"../runs/{datetime.now().strftime('%Y%m%d=%H%M%S')}\"\n",
    "callb = callBacks(log_dir, embedding_dataset, class_names)\n",
    "\n",
    "with strategy.scope(): \n",
    "    \n",
    "    input_emb = tf.keras.Input(shape=image_shape)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32, (3,3), strides=2)(input_emb)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, (3,3), strides=2)(x)  # Strided conv replaces pooling\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(128, (3,3), strides=2)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "\n",
    "    # x = tf.keras.layers.Conv2D(256, (3,3), strides=2)(x)\n",
    "    # x = tf.keras.layers.BatchNormalization()(x)\n",
    "    # x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "\n",
    "\n",
    "    feature_extractor = Model(inputs=input_emb, outputs=x, name=\"feature_extractor\")\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    loss_fc = contrastiveLoss(margin=0.5, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE )\n",
    "    \n",
    "    s_network = siameseNetwork(feature_extractor, image_shape, distance_metric=\"euclidean_distance\", embedding_size=128)\n",
    "\n",
    "    s_network.compile(optimizer=optimizer,\n",
    "                      loss = loss_fc)\n",
    "    \n",
    "    s_network.fit(training_dataset, epochs=50, validation_data=validation_dataset, callbacks=[callb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05995405-c18a-4067-867f-71861645ab61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca7998c-40de-4b65-a4c6-f95cc8484600",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV3Large(include_top = False,\n",
    "                              weights = None,\n",
    "                              input_shape=image_shape,\n",
    "                              alpha = 1.0,\n",
    "                              pooling = None,\n",
    "                              dropout_rate = 0.1)\n",
    "\n",
    "# num_layers = len(base_model.layers)\n",
    "# freeze_count = int(0.30 * num_layers)\n",
    "\n",
    "# for i, layer in enumerate(base_model.layers):\n",
    "#     if i < freeze_count:\n",
    "#         layer.trainable = False  # freeze\n",
    "#     else:\n",
    "#         layer.trainable = True   # unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5bf907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model = s_network.embedding_model\n",
    "log_dir = f\"../runs/{datetime.now().strftime('%Y%m%d=%H%M%S')}\"\n",
    "callb = callBacks(log_dir, embedding_dataset, class_names)\n",
    "\n",
    "with strategy.scope(): \n",
    "    optimizer = Adam(learning_rate=0.00001)\n",
    "    loss_fc = contrastiveLoss(margin=0.5, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE )\n",
    "    \n",
    "    feature_extractor = MobileNetV3Large(include_top = False,\n",
    "                                        weights = None,\n",
    "                                        input_shape=image_shape,\n",
    "                                        alpha = 1.0,\n",
    "                                        pooling = None,\n",
    "                                        dropout_rate = 0.1)\n",
    "    \n",
    "    s_network = siameseNetwork(feature_extractor, image_shape, distance_metric=\"cosine_distance\", embedding_size=512)\n",
    "\n",
    "    s_network.compile(optimizer=optimizer,\n",
    "                      loss = loss_fc)\n",
    "    \n",
    "    s_network.fit(training_dataset, epochs=15, validation_data=validation_dataset, callbacks=[callb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04140363-9848-45e9-beb7-174b6113be31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Nasnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f22b5d-0552-4069-bd1a-39308c145edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = NASNetMobile(input_shape=image_shape,\n",
    "                         include_top=False,\n",
    "                         weights=None,\n",
    "                         input_tensor=None,\n",
    "                         pooling=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2ffa6-7810-4a8e-a16e-fbaf6d143b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = regularizers.l2(0.01)\n",
    "base_embedding = createEmbedding(base_model, image_shape)\n",
    "siamese_network = createSiameseNetwork(base_embedding, image_shape)\n",
    "\n",
    "siamese_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e54348-e6ca-432a-b91a-ba6f64636fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.0001)\n",
    "loss_fc = contrastive_loss\n",
    "acc_metric = BinaryAccuracy(threshold=0.5)\n",
    "\n",
    "fit(model = siamese_network,\n",
    "    embedding_model = base_embedding,\n",
    "    training_dataset=training_dataset,\n",
    "    validation_dataset=validation_dataset,\n",
    "    embedding_dataset = embedding_dataset,\n",
    "    class_names = class_names,\n",
    "    epochs = 30,\n",
    "    loss_fc = loss_fc ,\n",
    "    optimizer = optimizer,\n",
    "    acc_metric = acc_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a48e8ab-5aa1-4a6c-928c-a838e5b2d8bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02cbad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model = s_network.embedding_model\n",
    "log_dir = f\"../runs/{datetime.now().strftime('%Y%m%d=%H%M%S')}\"\n",
    "callb = callBacks(log_dir, embedding_dataset, class_names)\n",
    "\n",
    "with strategy.scope(): \n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    loss_fc = contrastiveLoss(margin=0.5, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE )\n",
    "    # loss_fc = circleLoss(margin = 0.25, gamma = 64, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "    \n",
    "    feature_extractor = ResNet50V2(input_shape=image_shape,\n",
    "                                   include_top=False,\n",
    "                                   weights=\"imagenet\",\n",
    "                                   input_tensor=None,\n",
    "                                   pooling=None)\n",
    "    \n",
    "    # for layer in feature_extractor.layers[:60]:\n",
    "    #     layer.trainable = False\n",
    "    \n",
    "    s_network = siameseNetwork(feature_extractor, image_shape, distance_metric=\"cosine_distance\", embedding_size=512)\n",
    "\n",
    "    s_network.compile(optimizer=optimizer,\n",
    "                      loss = loss_fc)\n",
    "    \n",
    "    s_network.fit(training_dataset, epochs=15, validation_data=validation_dataset, callbacks=[callb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe5eba6-4cc0-4f6b-b0f8-3ac80a9f714b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c06026f-8f84-4dfa-854c-7a117b46ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(input_shape=image_shape,\n",
    "                         include_top=False,\n",
    "                         weights=None,\n",
    "                         input_tensor=None,\n",
    "                         pooling=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e454585b-370c-41f0-bf0f-55e8954aee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_embedding = createEmbedding(base_model, image_shape)\n",
    "base_embedding.summary()\n",
    "siamese_network = createSiameseNetwork(base_embedding, image_shape)\n",
    "siamese_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ff9cf6-0968-43cf-b584-3bf3248e799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.0001)\n",
    "loss_fc = contrastive_loss\n",
    "acc_metric = BinaryAccuracy(threshold=0.5)\n",
    "\n",
    "fit(model = siamese_network,\n",
    "    embedding_model = base_embedding,\n",
    "    training_dataset=training_dataset,\n",
    "    validation_dataset=validation_dataset,\n",
    "    embedding_dataset = embedding_dataset,\n",
    "    class_names = class_names,\n",
    "    epochs = 100,\n",
    "    loss_fc = loss_fc ,\n",
    "    optimizer = optimizer,\n",
    "    acc_metric = acc_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b6a60c-70e3-45a1-956f-fba96166af85",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##  Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e6897-18c9-41ed-9d51-502468eb6a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_weights = siamese_network.get_weights()\n",
    "path = \"../runs/20250204-102132/siamese_model_epoch_20.h5\"\n",
    "siamese_network.load_weights(path)\n",
    "new_weights = siamese_network.get_weights()\n",
    "\n",
    "print(\"Mean of old weights[0]:\", old_weights[0].mean())\n",
    "print(\"Mean of new weights[0]:\", new_weights[0].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be8885-672e-4704-bc8a-cf33e1895f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset\n",
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5594abb-2e5c-42b0-9599-9efb892db6f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc = evaluate(siamese_network, validation_dataset, 0.39) #0.24 on training set\n",
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
