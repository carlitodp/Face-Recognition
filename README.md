# Introduction

In this project, we implement a CNN-based Siamese network for face verification using TensorFlow. We train the model on the CelebA dataset, which contains over 200,000 images of more than 10,000 unique identities. The following sections detail the complete workflow, including the generation of positive and negative pairs, the model architecture, the evaluation metrics, the loss functions, and the experimental results.

# Dataset Download and Pair forming

The CelebA dataset can be downloaded from this https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html or loaded directly using TensorFlow’s API available https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/celeba/load_data. For our implementation, we downloaded the dataset and used the identities annotation file to assign a unique ID to each photo.

Positive pairs were generated by computing all possible combinations of photos belonging to the same identity, and a random sample was then extracted from this pool. Negative pairs were created by randomly pairing photos from different identities.

<img width="624" alt="image" src="https://github.com/user-attachments/assets/3e046272-d632-4f8b-a6cb-47b3fe869634" />

A data loader was implemented to manage the features described above. It accepts several parameters, including:

- path: The dataset folder path. This folder should contain one directory per identity, with each subdirectory holding the respective images.
- image_shape: The desired shape for the images (96,96,3) here.
- batch_size: The number of samples per batch.
- max_pos_pairs: The maximum number of positive pairs to sample. The number of negative pairs is set to match this value, ensuring balance between positive and negative pairs.
- apply_augmentation: A boolean flag indicating whether to apply data augmentation.
- preprocess_pipeline: A preprocessing function used when working with a TensorFlow model.
- normalize: A boolean flag that specifies whether to scale the data to the range [0, 1].
- image_per_celeb: The number of images per celebrity to use when forming positive pairs.

More details can be found in the data_loader.py file.

# Modeling

## Loss Function
To achieve this, we employed contrastive loss—a function that minimizes the distance between positive pairs while maximizing the distance between negative pairs. This loss function uses a distance metric (such as Euclidean or cosine distance) to quantify the similarity between embeddings. In our implementation, we used cosine distance (calculated as 1 - cosine similarity).

A critical parameter in this loss is the margin. The margin defines a threshold: for similar pairs, the loss encourages their distance to be smaller than the margin, while for dissimilar pairs, it enforces that their distance is at least greater than the margin. This helps ensure that the model effectively brings similar pairs closer together and pushes dissimilar pairs apart.

see contrastive_loss.py for more details.

## Metrics
To assess the performance of the model, we use several metrics to ensure that distances for similar pairs remain close to zero, while distances for dissimilar pairs are maximized. Specifically, we evaluate:

- Aggregated dissimilar distance
- Aggregated similar distance
- Threshold-based accuracy

see distance.py and accuracy.py for more details.

## Model

Using contrastive loss with image pairs, the network requires two input images. Each image is processed by the same embedding model—a CNN that acts as a feature extractor—to produce a vector representation (embedding) of the image. The network then computes the distance between these two embeddings, and this distance serves as the model's output.

An essential consideration in this process is the size of the embedding. The embedding size is paramount because it directly influences the model's ability to capture discriminative features. A low-dimensional embedding may not capture all critical information from the images, leading to underfitting where important details are missed. Conversely, an excessively high-dimensional embedding can result in the curse of dimensionality, causing the model to overfit by capturing noise and irrelevant variations rather than generalizable features.

For our baseline, we start with an embedding size of 128. This value provides a balanced starting point, although further tuning may be necessary to optimize performance for the specific task.


<img width="841" alt="siamese" src="https://github.com/user-attachments/assets/0f05a298-0d43-4793-befa-c9c4979c9949" />


For this purpose, we will compare two different CNN approaches: one implemented from scratch and another using an open-source architecture (e.g., ResNet or GoogLeNet). This comparison will help us evaluate the impact of each architecture on the overall performance of the face verification task.

see siamese_network.py for more details

# Results

## Customed CNN

| **Layer (type)**                                      | **Output Shape**         | **Param #** |
|-------------------------------------------------------|--------------------------|-------------|
| input_3 (InputLayer)                                  | [(None, 96, 96, 3)]       | 0           |
| conv2d_8 (Conv2D)                                     | (None, 47, 47, 16)       | 448         |
| batch_normalization_13 (BatchNormalization)         | (None, 47, 47, 16)       | 64          |
| activation_13 (Activation)                            | (None, 47, 47, 16)       | 0           |
| conv2d_9 (Conv2D)                                     | (None, 23, 23, 32)       | 4640        |
| batch_normalization_14 (BatchNormalization)         | (None, 23, 23, 32)       | 128         |
| activation_14 (Activation)                            | (None, 23, 23, 32)       | 0           |
| conv2d_10 (Conv2D)                                    | (None, 11, 11, 64)       | 18496       |
| batch_normalization_15 (BatchNormalization)         | (None, 11, 11, 64)       | 256         |
| activation_15 (Activation)                            | (None, 11, 11, 64)       | 0           |
| conv2d_11 (Conv2D)                                    | (None, 5, 5, 128)        | 73856       |
| batch_normalization_16 (BatchNormalization)         | (None, 5, 5, 128)        | 512         |
| activation_16 (Activation)                            | (None, 5, 5, 128)        | 0           |
| global_average_pooling2d_3 (GlobalAveragePooling2D)   | (None, 128)              | 0           |
| dense_7 (Dense)                                       | (None, 512)              | 66048       |
| batch_normalization_17 (BatchNormalization)         | (None, 512)              | 2048        |
| activation_17 (Activation)                            | (None, 512)              | 0           |
| dense_8 (Dense)                                       | (None, 256)              | 131328      |
| batch_normalization_18 (BatchNormalization)         | (None, 256)              | 1024        |
| activation_18 (Activation)                            | (None, 256)              | 0           |
| dense_9 (Dense)                                       | (None, 128)              | 32896       |
| **Total Parameters**                                  |                          | **331,744** |

compiled with:
- Contrastive Loss with margin = 1.0
- Aggregated dissimilar distance
- Aggregated similar distance
- Threshold-based accuracy (Threshold = 0.5)
- Adam optimizer with a learning rate of 1e-4
- Distance metric: Cosine Distance [-1, 1]


  ![image](https://github.com/user-attachments/assets/6aa93210-5750-44e6-a419-a7d60b84b858) ![image](https://github.com/user-attachments/assets/418cd7b7-7b63-4392-adcf-0aa631c529bf) ![image](https://github.com/user-attachments/assets/e92ae6e1-6626-4c78-931b-0af6c1993c8f) ![image](https://github.com/user-attachments/assets/614975ef-d91b-4b9c-95a2-ee13ffbeb3cd)

Over the epochs, the distances for negative pairs steadily increase while those for positive pairs decrease, resulting in a consistently declining loss. However, the significant gap between the training and validation metrics for both loss and accuracy indicates that the model is overfitting to the training data.




