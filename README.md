# Introduction

In this project, we implement a CNN-based Siamese network for face verification using TensorFlow. We train the model on the CelebA dataset, which contains over 200,000 images of more than 10,000 unique identities. The following sections detail the complete workflow, including the generation of positive and negative pairs, the model architecture, the evaluation metrics, the loss functions, and the experimental results.

# Dataset Download and Pair forming

The CelebA dataset can be downloaded from this https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html or loaded directly using TensorFlow’s API available https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/celeba/load_data. For our implementation, we downloaded the dataset and used the identities annotation file to assign a unique ID to each photo.

Positive pairs were generated by computing all possible combinations of photos belonging to the same identity, and a random sample was then extracted from this pool. Negative pairs were created by randomly pairing photos from different identities.

<img width="624" alt="image" src="https://github.com/user-attachments/assets/3e046272-d632-4f8b-a6cb-47b3fe869634" />

A data loader was implemented to manage the features described above. It accepts several parameters, including:

- path: The dataset folder path. This folder should contain one directory per identity, with each subdirectory holding the respective images.
- image_shape: The desired shape for the images.
- batch_size: The number of samples per batch.
- max_pos_pairs: The maximum number of positive pairs to sample. The number of negative pairs is set to match this value, ensuring balance between positive and negative pairs.
- apply_augmentation: A boolean flag indicating whether to apply data augmentation.
- preprocess_pipeline: A preprocessing function used when working with a TensorFlow model.
- normalize: A boolean flag that specifies whether to scale the data to the range [0, 1].
- image_per_celeb: The number of images per celebrity to use when forming positive pairs.

More details can be found in the data_loader.py file.

# Modeling

## Loss Function
To achieve this, we employed contrastive loss—a function that minimizes the distance between positive pairs while maximizing the distance between negative pairs. This loss function uses a distance metric (such as Euclidean or cosine distance), and in our implementation, we used the cosine distance (calculated as 1 - cosine similarity).

see contrastive_loss.py for more details.

## Metrics
To assess the performance of the model, we use several metrics to ensure that distances for similar pairs remain close to zero, while distances for dissimilar pairs are maximized. Specifically, we evaluate:

- Aggregated dissimilar distance
- Aggregated similar distance
- Threshold-based accuracy

see distance.py and accuracy.py for more details.

## Model

Using contrastive loss to work with image pairs, the network requires two input images. Each image is processed by the same embedding model—a CNN that serves as a feature extractor—which outputs a vector representation of the image. The network then computes the distance between these two embedding vectors, and this distance serves as the model's output.


<img width="841" alt="siamese" src="https://github.com/user-attachments/assets/0f05a298-0d43-4793-befa-c9c4979c9949" />


For this purpose, we will compare two different CNN approaches: one implemented from scratch and another using an open-source architecture (e.g., ResNet or GoogLeNet). This comparison will help us evaluate the impact of each architecture on the overall performance of the face verification task.

# Results
  


