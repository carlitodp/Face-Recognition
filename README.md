# Introduction

In this project, we implement a CNN-based Siamese network for face verification using TensorFlow. We train the model on the CelebA dataset, which contains over 200,000 images of more than 10,000 unique identities. The following sections detail the complete workflow, including the generation of positive and negative pairs, the model architecture, the evaluation metrics, the loss functions, and the experimental results.

# Dataset Download and Pair forming

The CelebA dataset can be downloaded from this https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html or loaded directly using TensorFlow’s API available https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/celeba/load_data. For our implementation, we downloaded the dataset and used the identities annotation file to assign a unique ID to each photo.

Positive pairs were generated by computing all possible combinations of photos belonging to the same identity, and a random sample was then extracted from this pool. Negative pairs were created by randomly pairing photos from different identities.

<img width="624" alt="image" src="https://github.com/user-attachments/assets/3e046272-d632-4f8b-a6cb-47b3fe869634" />

A data loader was implemented to manage the features described above. It accepts several parameters, including but not limited to:

- path: The dataset folder path. This folder should contain one directory per identity, with each subdirectory holding the respective images.
- image_shape: The desired shape for the images (128,128,3) here.
- batch_size: The number of samples per batch.
- max_pos_pairs: The maximum number of positive pairs to sample. The number of negative pairs is set to match this value, ensuring balance between positive and negative pairs.
- apply_augmentation: A boolean flag indicating whether to apply data augmentation.
- preprocess_pipeline: A preprocessing function used when working with a TensorFlow model.
- normalize: A boolean flag that specifies whether to scale the data to the range [0, 1].
- image_per_celeb: The number of images per celebrity to use when forming positive pairs.

More details can be found in the data_loader.py file.

# Modeling

## Loss Function
To achieve this, we employed contrastive loss—a function that minimizes the distance between positive pairs while maximizing the distance between negative pairs. This loss function uses a distance metric (such as Euclidean or cosine distance) to quantify the similarity between embeddings. In our implementation, we used euclidean distance.

A critical parameter in this loss is the margin. The margin defines a threshold: for similar pairs, the loss encourages their distance to be smaller than the margin, while for dissimilar pairs, it enforces that their distance is at least greater than the margin. This helps ensure that the model effectively brings similar pairs closer together and pushes dissimilar pairs apart.

see contrastive_loss.py for more details.

## Metrics
To assess the performance of the model, we use several metrics to ensure that distances for similar pairs remain close to zero, while distances for dissimilar pairs are maximized. Specifically, we evaluate:

- Aggregated dissimilar distance
- Aggregated similar distance
- Threshold-based accuracy

see distance.py and accuracy.py for more details.

## Model

Using contrastive loss with image pairs, the network requires two input images. Each image is processed by the same embedding model—a CNN that acts as a feature extractor—to produce a vector representation (embedding) of the image. The network then computes the distance between these two embeddings, and this distance serves as the model's output.

An essential consideration in this process is the size of the embedding. The embedding size is paramount because it directly influences the model's ability to capture discriminative features. A low-dimensional embedding may not capture all critical information from the images, leading to underfitting where important details are missed. Conversely, an excessively high-dimensional embedding can result in the curse of dimensionality, causing the model to overfit by capturing noise and irrelevant variations rather than generalizable features.

<img width="841" alt="siamese" src="https://github.com/user-attachments/assets/0f05a298-0d43-4793-befa-c9c4979c9949" />

For this purpose, we implemented a custom CNN and trained the model using different combinations of parameters, iteratively tuning each hyperparameter to find the optimal configuration for the face-verification task.

see siamese_network.py for more details

# Results

## Customed CNN

| # | Layer&nbsp;(type)                 | Output shape          | Param # |
|---|----------------------------------|-----------------------|---------|
| 1 | **InputLayer**                   | (None, 128, 128, 3)   | 0 |
| 2 | Conv2D `conv2d`                  | (None, 128, 128, 64)  | 4,864 |
| 3 | BatchNorm `batch_normalization`  | (None, 128, 128, 64)  | 256 |
| 4 | LeakyReLU `leaky_re_lu`          | (None, 128, 128, 64)  | 0 |
| 5 | Conv2D `conv2d_1`                | (None, 128, 128, 64)  | 4,160 |
| 6 | BatchNorm `batch_normalization_1`| (None, 128, 128, 64)  | 256 |
| 7 | LeakyReLU `leaky_re_lu_1`        | (None, 128, 128, 64)  | 0 |
| 8 | MaxPool2D `max_pooling2d`        | (None, 64, 64, 64)    | 0 |
| 9 | Conv2D `conv2d_2`                | (None, 64, 64, 96)    | 55,392 |
| 10| BatchNorm `batch_normalization_2`| (None, 64, 64, 96)    | 384 |
| 11| LeakyReLU `leaky_re_lu_2`        | (None, 64, 64, 96)    | 0 |
| 12| Conv2D `conv2d_3`                | (None, 64, 64, 96)    | 9,312 |
| 13| BatchNorm `batch_normalization_3`| (None, 64, 64, 96)    | 384 |
| 14| LeakyReLU `leaky_re_lu_3`        | (None, 64, 64, 96)    | 0 |
| 15| MaxPool2D `max_pooling2d_1`      | (None, 32, 32, 96)    | 0 |
| 16| Conv2D `conv2d_4`                | (None, 32, 32, 128)   | 110,720 |
| 17| BatchNorm `batch_normalization_4`| (None, 32, 32, 128)   | 512 |
| 18| LeakyReLU `leaky_re_lu_4`        | (None, 32, 32, 128)   | 0 |
| 19| Conv2D `conv2d_5`                | (None, 32, 32, 128)   | 16,512 |
| 20| BatchNorm `batch_normalization_5`| (None, 32, 32, 128)   | 512 |
| 21| LeakyReLU `leaky_re_lu_5`        | (None, 32, 32, 128)   | 0 |
| 22| MaxPool2D `max_pooling2d_2`      | (None, 16, 16, 128)   | 0 |
| 23| Conv2D `conv2d_6`                | (None, 16, 16, 256)   | 295,168 |
| 24| BatchNorm `batch_normalization_6`| (None, 16, 16, 256)   | 1,024 |
| 25| LeakyReLU `leaky_re_lu_6`        | (None, 16, 16, 256)   | 0 |
| 26| MaxPool2D `max_pooling2d_3`      | (None, 8, 8, 256)     | 0 |
| 27| Conv2D `conv2d_7`                | (None, 8, 8, 256)     | 65,792 |
| 28| BatchNorm `batch_normalization_7`| (None, 8, 8, 256)     | 1,024 |
| 29| LeakyReLU `leaky_re_lu_7`        | (None, 8, 8, 256)     | 0 |
| 30| MaxPool2D `max_pooling2d_4`      | (None, 4, 4, 256)     | 0 |
| 31| Conv2D `conv2d_8`                | (None, 2, 2, 512)     | 1,180,160 |
| 32| BatchNorm `batch_normalization_8`| (None, 2, 2, 512)     | 2,048 |
| 33| LeakyReLU `leaky_re_lu_8`        | (None, 2, 2, 512)     | 0 |
| 34| MaxPool2D `max_pooling2d_5`      | (None, 1, 1, 512)     | 0 |
| 35| GlobalAvgPool2D                  | (None, 512)           | 0 |
| 36| Dense `dense`                    | (None, 32)            | 16,416 |
|   | **Totals**                       |                       | **1,764,896** |


compiled with:
- Contrastive Loss with margin = 0.5
- Aggregated dissimilar distance
- Aggregated similar distance
- Threshold-based accuracy (Threshold = 0.5)
- Adam optimizer with a learning rate of 1e-4
- Distance metric: Euclidean Distance, with normalized embeddings.

Additionally:
- Data: 1.6M pairs (800k positives and 800k negatives)
- Embedding size: 32
- Epoch: 10
- Augmentation: False

<img width="400" height="320" alt="image" src="https://github.com/user-attachments/assets/3b9c07b0-1cc0-4bc5-975c-d422fd22a145" /> <img width="400" height="320"  alt="image" src="https://github.com/user-attachments/assets/a0b7c806-08f2-4c94-be3b-9df679122fcf" /> <img width="400" height="320"  alt="image" src="https://github.com/user-attachments/assets/b987ad54-02a1-4b64-aa02-10f0dc3f8490" /> <img width="400" height="320"  alt="image" src="https://github.com/user-attachments/assets/550513bf-3117-4eae-bc57-c5a94e129d81" />

The loss and distance curves don’t show a clear trend at their current scale. Plotting the metrics per iteration—or zooming in on individual epochs—should reveal what’s really going on.

<img width="400" height="320" alt="image" src="https://github.com/user-attachments/assets/8a379cd2-4d83-4981-b528-32a32ac1f714" />
<img width="400" height="320" alt="image" src="https://github.com/user-attachments/assets/a6ac9de4-8513-48ca-87dd-70eeef473d0d" /> <img width="400" height="320" alt="image" src="https://github.com/user-attachments/assets/5bc36c03-621d-464c-a8eb-562c01c95dbb" />


Across 15 epochs we observed three distinct patterns:

- Dissimilar-pair distance keeps climbing—exactly what we want, because negatives should move farther apart in embedding space.

- Loss and similar-pair distance drop until ≈ 120 k iterations, then start to rise. That reversal is a classic early sign of over-fitting.

- Accuracy plateaus at ~ 90 % on both train and validation data by epoch 10.

To avoid the over-fit phase we introduced early stopping at epoch 10 and will explore stronger regularisation (heavier augmentation, dropout, etc.). Future gains may come from swapping in a pre-trained backbone, adding more data, or further hyper-parameter tuning.

Note: 
- The repo contains an Excel workbook that logs every trial and shows how each hyper-parameter affected loss, distance metrics, and accuracy.
- More updates are on the way !



